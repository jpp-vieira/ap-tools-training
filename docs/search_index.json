[["index.html", "Analytical Platform and related tools training 1 Summary of available resources 1.1 R mentoring 1.2 Internal R (and SQL) Training Group materials 1.3 Coffee and Coding and Bitesize sessions 1.4 Analytical Function training opportunities 1.5 Datacamp 1.6 Other assistance", " Analytical Platform and related tools training MoJ coding training leads 2021-08-26 1 Summary of available resources 1.1 R mentoring It is recommended that all who are new to R or DASD request an R mentor. The purpose of the scheme is to provide a better on-the-job R learning experience and raise awareness of the preferred DASD ways of working that will for instance enable people to get up to speed more quickly with others’ code. The scheme is also open to non-coders who need to use the Analytical Platform to advise them through the learning process, and for those commencing a more complex project involving the use of R. To request an R mentor please complete this mentee form. If you could become an R mentor (we have a shortage of mentors) please complete this mentor form. For more information please contact Jessica Dawson. 1.2 Internal R (and SQL) Training Group materials The live R, SQL and GitHub training sessions are generally run twice or three times each year. However, recordings are available so you can work through yourself. These sessions are run using the MoJ Analytical Platform so they are recommended for those working in DASD (or with the Analytical Platform). You can view the material at: • Introduction to R: https://github.com/moj-analytical-services/IntroRTraining • R Charting: https://github.com/moj-analytical-services/ggplotTraining • Introduction to GitHub: https://github.com/moj-analytical-services/git-training-class • R Markdown: https://github.com/moj-analytical-services/rmarkdown_training • Interfacing Excel with R: https://github.com/moj-analytical-services/r-excel-training • Writing Functions in R: https://github.com/moj-analytical-services/writing_functions_in_r • Introduction to SQL: https://github.com/moj-analytical-services/sql_training The recordings of the R and SQL training sessions (the GitHub ones have not yet been recorded) are stored on MS Stream in the links below: • R training MS Stream Channel: https://web.microsoftstream.com/channel/aa3cda5d-99d6-4e9d-ac5e-6548dd55f52a • SQL training MS Stream Channel: https://web.microsoftstream.com/channel/7cd1cdaf-79cb-4e1e-ab2b-448d8f69f6a1 One great way of learning is by teaching. If you would be interested in being part of the R (and SQL) training group, whether booking courses/allocating places, designing training, or presenting, please contact Aidan Mews or Georgina Eaton. If you have any questions please contact Aidan Mews or Georgina Eaton. 1.3 Coffee and Coding and Bitesize sessions The internal R/SQL/Github training (see above) is complemented by coffee and coding and bitesize sessions (click on the links to access the recordings). The contacts are Katharine Breeze and George Papadopoulos respectively. 1.4 Analytical Function training opportunities There are now many technical Analytical Function training opportunities for analysts including about topics not presently covered internally e.g. an introduction to python. Examples useful for RAP practitioners include Best practice in programming – clean code – GSS (civilservice.gov.uk) and an Introduction to unit testing – GSS (civilservice.gov.uk). You can view such opportunities via: • A user-friendly list (but not updated since Jan 2021): https://www.gov.uk/guidance/af-learning-curriculum-technical • GSS Training Courses (many which are open to all analysts): https://gss.civilservice.gov.uk/training-courses/ 1.5 Datacamp Paid Datacamp licenses are beneficial to cover gaps in current training provision that are not picked up by either internal or GSS/Analytical function training currently e.g. more advanced R, SQL, and python skills. You can read more at https://www.datacamp.com/. If you would like to have a Datacamp license please get approval from your deputy director and contact Aidan Mews. 1.6 Other assistance Technical help can be requested via the following DASD slack channels: • intro_R which provides support to those starting out in the world of R and RAP. • R which is for beginners and experts alike. • sql • python You may also find useful a trello board providing Links to further free online analytical training including in R and R cheatsheets. "],["rap_struc.html", "2 RAP Structure 2.1 Use one repo for each endpoint in your process 2.2 Common structure to a RAP repo", " 2 RAP Structure In order to develop a pipeline like the one above, certain ways of structuring your RAP project can help. 2.1 Use one repo for each endpoint in your process As in the diagram above, define a set of endpoints or outputs that your process will produce. This could include: Cleaned datasets for internal use Publishable datasets (these may be the same as those above) Publication outputs (Charts, Tables, Publication text) Separate repos for other outputs (eg. MI packs, briefing packs, data visualisation tools) This makes it easier to use outputs for multiple purposes, rather than having to extract them from the middle of a larger process. It also allows other users who want to adapt your code or outputs to find the section of code that they need, without having to understand the full range of code. In the diagram above, the aim of the first repository should be to render the data into a format that can be used to produce the broadest range of other outputs, which are then each created within their own repositories. You should also consider the point at which you want to include disclosure control in the process. For example, the first repo might create a full, unredacted dataset for internal use. You may they want to include a further stage which aggregates data to prevent small numbers being generated. 2.2 Common structure to a RAP repo A RAP repository should have: An ‘inputs’ folder, for any inputs required by the process. This should only be used for things like lookup tables, templates etc. Datasets should be stored in s3. An ‘outputs’ folder, for all outputs (eg. HTML, pdf, png) generated by the process. A ‘processing’ folder containing R scripts or Rmd files used in the process. A ‘functions’ folder within the processing folder, containing any functions created for the process. A RAP repository should not have: Stored datasets General functions. These should be stored in a package – ideally mojrap, or a process-specific package if necessary. Note that this structure only applies to a RAP repo that works as a series of scripts run manually. You may wish to create your repo as a package, in which case you should adopt the required structure for a package. "],["git_flow.html", "3 Version Control with Git Flow", " 3 Version Control with Git Flow Git Flow is an ideal workflow for Git and GitHub to allow collaboration and development of new features. It is based on three types of branch: Master – the live version of the code, managed by an administrator. No editing is done on this branch. Development – a staging area for testing finished features together, before getting merged into the live master branch Features – these branches are used for individual new features. When a new feature is worked on, before testing it with other new features it gets developed on its own branch, which can be collaborated on by multiple people. When a feature is finished and had unit testing completed, it can be merged into the development branch. This will also feed in to the categorisation of a release within GitHub. Within RAP, each release should align with a finalised process for the production of a single publication. This means that in future we can easily revert code to its state at the point of publication and easily recreate old data if needed. For more information, see Learn to love Github with Git Flow. "],["data_struc.html", "4 Data Structures", " 4 Data Structures When developing RAP processes, you should ensure that your data adheres to tidy data formats at each stage. At its core this means adhering to three basic principles: Each variable must have its own column Each observation must have its own row Each value must have its own cell In most cases, this will also mean only having one column of values, while every other column is used to define that value. For example, if you were creating a dataset showing the prison population by age over time, it might look like this: Pop_2017 Pop_2018 Pop_2020 15-17 470 3540 70706 18-20 444 3241 68861 21+ 409 3192 69142 This way of arranging a dataset is not tidy because each observation doesn’t have its own row. Each row contains an observation for 2017, 2018 and 2019. To convert this dataset to tidy data, you would rearrange it to the following: Age Year Population 15-17 2017 470 18-20 2017 3540 21+ 2017 70706 15-17 2018 444 18-20 2018 3241 21+ 2018 68861 15-17 2019 409 18-20 2019 3192 21+ 2019 69142 As you can see from the above example, turning your dataset into tidy data can be thought of as making it longer and thinner, with a single column of values. There are three main advantages to following a tidy data structure within RAP: Picking one consistent way of storing data allows you to learn how to use tools and packages within R and apply them across multiple datasets. It also enables you to use and develop functions and packages that will work across multiple datasets because you know how the data will be structured. Commonly used tools like dplyr and ggplot2 are designed to work with tidy data. R is designed to work with vectorised data. Placing variables in columns transforms your data into a set of vectors and so it will be easier to work with within R. For these reasons it is desirable to get your data into tidy format as soon as possible within your RAP process so you can start using some of these advantages as early in the process as possible. Sometimes you may want to output data in an ‘untidy’ format, such as for outputs that are to be looked at by a user but not to be read by further code. Make sure to have these be the final output rather than datasets that are then manipulated by your code. All data wrangling should be done with tidy data. For more information on tidy data, see https://r4ds.had.co.nz/tidy-data.html "],["generalisable_code.html", "5 Generalisable Code", " 5 Generalisable Code When writing code, you should consider how to make it as generalisable as possible. That is, you should write your code in a way that allows yourself and others to take what you’ve written and use it for several different purposes. In practice, this means writing code as functions within R. A function is a set of commands that are bundled together so that they can all be repeated with a single line of code. A function will generally accept different inputs, or parameters, which allows it to be applied in different situations. Within R, functions can be grouped together in packages. This means writing your code as much as possible to use these three methods (in reverse order of preference): Custom functions Custom packages Multi-use packages (eg. mojrap) Code should use as few individual lines of processing as possible. You should design your process to make use of functions as much as possible and these functions should be designed to work on other datasets than those within your RAP process. When creating functions, make them as generalisable as possible. This means not including any code within your function that is only applicable to your data and not hardcoding any dataset-specific variables. Any references to data should be set within the parameters of the function so that the function can work on other datasets. Code that is used to recode variables, for example, should refer to a lookup table rather than hard-coding the recodes within the function. This means that your function can be used in other contexts by simply referring to a new lookup table. These functions should be built into packages, either custom packages for your particular RAP, or packages such as mojrap which work across multiple processes. Mojrap is intended to serve as the repository for functions that can be used across publications within MOJ. It is also good practice to use the common packages used across tidy data to standardise your code, these could include: dplyr tidyverse openxlsx ggplot2 When writing your RAPped code in R, it is good practice to use package names with a double colon when calling functions from external packages. For example, when using dplyr’s filter function use dplyr::filter rather than just filter. This means whenever your code is reproduced, the packages required to run it are obvious from the functions used within the code. This also makes functions easier to code into packages if you do this later down the line. "],["standards.html", "6 Develop your process against a defined set of standards", " 6 Develop your process against a defined set of standards A RAP, standing for Reproducible Analytical Pipeline, is a production methodology for automating the steps involved in creating a statistical report. It is a series of practices that focus on reproducibility –at any point in the future we should be able to look back at this work and be able to reproduce everything that we have done today. There are various ways of measuring how well a RAP project meets these aims. The chart below shows a range of steps along that process, with Stages 4a onwards considered to be ‘RAP’. You may also wish to consider the RAP Minimum Viable Product Within all these methods, there is an emphasis on good coding practices. You should followe the DASD Coding Principles within your project. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
